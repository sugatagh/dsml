<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Higgs Boson Event Detection | Sugata Ghosh, PhD</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Binary Classification with Deep Learning">
    <meta name="generator" content="Hugo 0.121.2">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="https://sugatagh.github.io/dsml/ananke/css/main.min.css" >



    

    
      

    

    

    
      <link rel="canonical" href="https://sugatagh.github.io/dsml/projects/higgs-boson-event-detection/">
    

    <meta property="og:title" content="Higgs Boson Event Detection" />
<meta property="og:description" content="Binary Classification with Deep Learning" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://sugatagh.github.io/dsml/projects/higgs-boson-event-detection/" /><meta property="article:section" content="projects" />
<meta property="article:published_time" content="2022-05-29T22:01:00+05:30" />
<meta property="article:modified_time" content="2022-05-29T22:01:00+05:30" />

<meta itemprop="name" content="Higgs Boson Event Detection">
<meta itemprop="description" content="Binary Classification with Deep Learning"><meta itemprop="datePublished" content="2022-05-29T22:01:00+05:30" />
<meta itemprop="dateModified" content="2022-05-29T22:01:00+05:30" />
<meta itemprop="wordCount" content="5399">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Higgs Boson Event Detection"/>
<meta name="twitter:description" content="Binary Classification with Deep Learning"/>

	<style>
.has-mathjax {
    visibility: hidden;
}
</style>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<script>
window.MathJax = {
  startup: {
    pageReady: () => {
      return MathJax.startup.defaultPageReady().then(() => {
        for (let element of document.getElementsByClassName("has-mathjax")) {
            element.style.visibility = "visible"
        }
      });
    }
  }
};
</script>
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  
  <header class="cover bg-top" style="background-image: url('https://sugatagh.github.io/dsml/images/bg-projects/bg-hbed-1.jpg');">
    <div class="bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://sugatagh.github.io/dsml/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Sugata Ghosh, PhD
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://sugatagh.github.io/dsml/about/" title="About page">
              About
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://sugatagh.github.io/dsml/overview/" title="Overview page">
              Overview
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://sugatagh.github.io/dsml/certifications/" title="Certifications page">
              Certifications
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://sugatagh.github.io/dsml/projects/" title="Projects page">
              Projects
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://sugatagh.github.io/dsml/notes/" title="Notes page">
              Notes
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://sugatagh.github.io/dsml/cv/" title="CV page">
              CV
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://sugatagh.github.io/dsml/contact/" title="Contact page">
              Contact
            </a>
          </li>
          
        </ul>
      
      
<div class="ananke-socials">
  
    
    <a href="https://www.linkedin.com/in/sugataghosh/" target="_blank" rel="noopener" class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" aria-label="follow on LinkedIn——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
    
    <a href="https://twitter.com/sugatagh" target="_blank" rel="noopener" class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Twitter link" aria-label="follow on Twitter——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
    
    <a href="https://github.com/sugatagh" target="_blank" rel="noopener" class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" aria-label="follow on GitHub——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
    
    <a href="https://www.kaggle.com/sugataghosh" target="_blank" rel="noopener" class="Kaggle ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Kaggle link" aria-label="follow on Kaggle——Opens in a new window">
      
        Kaggle
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
</div>

    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          <div class="f2 f1-l fw2 white-90 mb0 lh-title">Higgs Boson Event Detection</div>
          
            <div class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
              Binary Classification with Deep Learning
            </div>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Projects
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
      
      <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://sugatagh.github.io/dsml/projects/higgs-boson-event-detection/&amp;title=Higgs%20Boson%20Event%20Detection" class="ananke-social-link linkedin no-underline" aria-label="share on LinkedIn">
        
        <span class="icon"> <svg style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span>
        
      </a>
    
      
      <a href="https://twitter.com/intent/tweet?url=https://sugatagh.github.io/dsml/projects/higgs-boson-event-detection/&amp;text=Higgs%20Boson%20Event%20Detection" class="ananke-social-link twitter no-underline" aria-label="share on Twitter">
        
        <span class="icon"> <svg style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>
</span>
        
      </a>
    
  </div>


      <h1 class="f1 athelas mt3 mb1">Higgs Boson Event Detection</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2022-05-29T22:01:00+05:30">May 29, 2022</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>In particle physics, events refer to the results just after a fundamental interaction took place between subatomic particles, occurring in a very short time span in a well-localized region of space. A <em>background event</em> is explained by the existing theories. On the other hand, a <em>signal event</em> indicates a process that cannot be described by previous observations and leads to the potential discovery of a new particle. In this project, we aim to predict if a given event is background or signal.</p>
<p><a href="https://github.com/sugatagh/Higgs-Boson-Event-Detection">GitHub repository</a></p>
<h3 id="-contents">○ Contents</h3>
<ul>
<li><a href="#-overview">Overview</a></li>
<li><a href="#-introduction">Introduction</a></li>
<li><a href="#-exploratory-data-analysis">Exploratory Data Analysis</a></li>
<li><a href="#-train-test-split">Train-Test Split</a></li>
<li><a href="#-data-preprocessing">Data Preprocessing</a></li>
<li><a href="#-feature-engineering">Feature Engineering</a></li>
<li><a href="#-baseline-model">Baseline Model</a></li>
<li><a href="#-hyperparameter-tuning">Hyperparameter Tuning</a></li>
<li><a href="#-prediction-and-evaluation">Prediction and Evaluation</a></li>
<li><a href="#-acknowledgements">Acknowledgements</a></li>
<li><a href="#-references">References</a></li>
</ul>
<h3 id="-overview">○ Overview</h3>
<ul>
<li>In <a href="https://en.wikipedia.org/wiki/Particle_physics">particle physics</a>, an <em>event</em> refers to the results just after a <a href="https://en.wikipedia.org/wiki/Fundamental_interaction">fundamental interaction</a> took place between <a href="https://en.wikipedia.org/wiki/Subatomic_particle">subatomic particles</a>, occurring in a very short time span, at a well-localized region of space.</li>
<li>A <em>background event</em> is explained by the existing theories and previous observations. On the other hand, a <em>signal event</em> indicates a process that cannot be described by previous observations and leads to the potential discovery of a new particle.</li>
<li>In this project, we aim to predict if a given event is <em>background</em> or <em>signal</em>, based on the data provided in the Kaggle competition <a href="https://www.kaggle.com/competitions/higgs-boson/">Higgs Boson Machine Learning Challenge</a>.</li>
<li>A detailed backdrop of the problem is given, and <a href="https://en.wikipedia.org/wiki/Exploratory_data_analysis">exploratory data analysis</a> on the provided data is carried out.</li>
<li>The observations obtained from EDA are used in the <a href="https://en.wikipedia.org/wiki/Data_Preprocessing">data preprocessing</a> and <a href="https://en.wikipedia.org/wiki/Feature_engineering">feature engineering</a> stages.</li>
<li>We build a <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a> and <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">tune</a> it to predict if a given event is <em>background</em> or <em>signal</em>.</li>
<li>We employ the <a href="https://www.kaggle.com/competitions/higgs-boson/overview/evaluation">approximate median significance</a> (AMS) metric to evaluate the models. The final model obtains a training AMS of <span class="has-mathjax">\(2.500144\)</span>
 and a test AMS of <span class="has-mathjax">\(1.200022\).</span>
 It achieves a training <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification">accuracy</a> of <span class="has-mathjax">\(0.827070\)</span>
 and a test accuracy of <span class="has-mathjax">\(0.824100\).</span>
</li>
</ul>
<h3 id="-introduction">○ Introduction</h3>
<ul>
<li><a href="#backstory">Backstory</a></li>
<li><a href="#data">Data</a></li>
<li><a href="#project-objective">Project Objective</a></li>
<li><a href="#evaluation-metric">Evaluation Metric</a></li>
</ul>
<h4 id="backstory">Backstory</h4>
<p><a href="https://en.wikipedia.org/wiki/Particle_accelerator">Particle accelerators</a> enable physicists to explore the fundamental nature of matter by observing subatomic particles produced by high-energy collisions of <a href="https://en.wikipedia.org/wiki/Particle_beam">particle beams</a>. The experimental measurements from these collisions inevitably lack precision, which is where <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> comes into picture. The research community typically relies on standardized machine learning software packages for the analysis of the data obtained from such experiments and spends a huge amount of effort towards improving statistical power by extracting features of significance, derived from the raw measurements.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Higgs_boson">Higgs boson</a> <a href="https://en.wikipedia.org/wiki/Elementary_particle">particle</a> has been observed to decay into <a href="https://en.wikipedia.org/wiki/Boson">boson</a> pairs. To establish that the Higgs field provides the interaction which gives mass to the fundamental <a href="https://en.wikipedia.org/wiki/Fermion">fermions</a>, it has to be demonstrated that the Higgs boson can decay into fermion pairs through direct <a href="https://en.wikipedia.org/wiki/Particle_decay">decay</a> modes. Subsequently, to seek evidence on the decay of Higgs boson into fermion pairs and to precisely measure their characteristics became one of the important lines of enquiry. Among the available modes, the most promising is the decay to a pair of <a href="https://simple.wikipedia.org/wiki/Tau_lepton">tau leptons</a> <span class="has-mathjax">\(h \to \tau^+\tau^-\).</span>
</p>
<p>The <a href="https://cds.cern.ch/record/1632191">first evidence of <span class="has-mathjax">\(h \to \tau^+\tau^-\)</span>
 decays</a> was reported in <span class="has-mathjax">\(2013\),</span>
 based on the full set of proton-proton collision data recorded by the <a href="https://home.cern/science/experiments/atlas">ATLAS</a> experiment at the LHC during <span class="has-mathjax">\(2011-2012\).</span>
 Despite the consistency of the data with <span class="has-mathjax">\(h \to \tau^+\tau^-\)</span>
 decays, it could not be ensured that the statistical power exceeds the <span class="has-mathjax">\(5\sigma\)</span>
 threshold, which is the required standard for claims of discovery in high-energy physics community.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-atlas-experiment.png"
         alt="Higgs into fermions: Evidence of the Higgs boson decaying to fermions (image credit: CERN)"/><figcaption>
            <p>Higgs into fermions: Evidence of the Higgs boson decaying to fermions (image credit: <a href="https://en.wikipedia.org/wiki/CERN">CERN</a>)</p>
        </figcaption>
</figure>

<p>The aim of the offline analysis is to find a <em>selection region</em> in the feature space that produces a significant excess of events compared to what known background processes can explain. These are called <em>signal events</em>. Once the region has been fixed, a statistical test is applied to determine the significance of the excess. If the probability that the excess has been produced by background processes falls below a certain limit, it indicates the discovery of a new particle.</p>
<p>The broad goal is to improve the procedure that produces a selection region, i.e., the region (not necessarily connected) in the feature space that produces signal events.</p>
<p>Machine learning plays a major role in processing data resulting from experiments at particle colliders. The ML classifiers learn to distinguish between different types of collision events by training on simulated data from sophisticated Monte-Carlo programs. Shallow neural networks with single hidden layer are one of the primary techniques used for this analysis.</p>
<p>Efforts to increase statistical power tend to focus on developing new features for use with the existing ML classifiers. These high-level features are non-linear functions of the low-level measurements, derived using knowledge of the underlying physical processes.</p>
<p>The abundance of labeled simulation training data and the complex underlying structure make this an ideal application for <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a>. Large <a href="https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks">deep neural networks</a> can simplify and improve the analysis of high-energy physics data by automatically learning high-level features from the data. In particular, they increase the statistical power of the analysis even without the help of manually derived high-level features.</p>
<h4 id="data">Data</h4>
<p><em>Source:</em> <a href="https://www.kaggle.com/competitions/higgs-boson/data">https://www.kaggle.com/competitions/higgs-boson/data</a></p>
<p>The <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets#Training_data_set">training set</a> contains <span class="has-mathjax">\(250000\)</span>
 observations of <span class="has-mathjax">\(33\)</span>
 columns.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-data-train.png"/>
</figure>

<p>An example of an observation from the training set is as follows.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-data-train-example.png"/>
</figure>

<p>The <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets#Test_data_set">test set</a> contains <span class="has-mathjax">\(550000\)</span>
 observations. It has all the columns of the training set except <code>Label</code> (signal or background) and <code>Weight</code>.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-data-test.png"/>
</figure>

<p>An observation from the test set typically looks like this.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-data-test-example.png"/>
</figure>

<h4 id="project-objective">Project Objective</h4>
<p>The objective of the project is to classify an event produced in the particle accelerator as <em>background</em> or <em>signal</em>. As described earlier, a background event is explained by the existing theories and previous observations. A signal event, however, indicates a process that cannot be described by previous observations and leads to the potential discovery of a new particle.</p>
<h4 id="evaluation-metric">Evaluation Metric</h4>
<p>The <a href="https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers">evaluation metric</a>, used in this project, is the approximate median significance (AMS), given by</p>
<div class="has-mathjax">

\[AMS := \sqrt{2\left(\left(s+b+b_r\right)\log{\left(1+\frac{s}{b+b_r}\right)}-s\right)},\]

</div>

<p>where</p>
<ul>
<li><span class="has-mathjax">\(s:\)</span>
 unnormalized <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">true positive rate</a>,</li>
<li><span class="has-mathjax">\(b:\)</span>
 unnormalized <a href="https://en.wikipedia.org/wiki/False_positive_rate">false positive rate</a>,</li>
<li><span class="has-mathjax">\(b_r = 10:\)</span>
 constant regularization term,</li>
<li><span class="has-mathjax">\(\log:\)</span>
 <a href="https://en.wikipedia.org/wiki/Natural_logarithm">natural logarithm</a>.</li>
</ul>
<p>Precisely, let <span class="has-mathjax">\((y_1, \ldots, y_n) \in \{\text{b},\text{s}\}^n\)</span>
 be the vector of true test labels <span class="has-mathjax">(\(\text{b}\)</span>
 indicating background event and <span class="has-mathjax">\(\text{s}\)</span>
 indicating signal event) and let <span class="has-mathjax">\((\hat{y}_1, \ldots, \hat{y}_n) \in \{\text{b},\text{s}\}^n\)</span>
 be the vector of predicted test labels. Also let <span class="has-mathjax">\((w_1, \ldots, w_n) \in {\mathbb{R}^+}^n\)</span>
 be the vector of weights, where <span class="has-mathjax">\(\mathbb{R}^+\)</span>
 is the set of positive real numbers. Then</p>
<div class="has-mathjax">

\[s = \sum_{i=1}^n w_i \mathbb{1}\left\{y_i = s\right\} \mathbb{1}\left\{\hat{y_i} = s\right\}\]

</div>

<p>and</p>
<div class="has-mathjax">

\[b = \sum_{i=1}^n w_i \mathbb{1}\left\{y_i = b\right\} \mathbb{1}\left\{\hat{y_i} = s\right\},\]

</div>

<p>where the <a href="https://en.wikipedia.org/wiki/Indicator_function">indicator function</a> <span class="has-mathjax">\(\mathbb{1}\left\{S\right\}\)</span>
 is <span class="has-mathjax">\(1\)</span>
 if <span class="has-mathjax">\(S\)</span>
 is true and <span class="has-mathjax">\(0\)</span>
 otherwise. See section <span class="has-mathjax">\(2.3\),</span>
 <span class="has-mathjax">\(2.4\),</span>
 and <span class="has-mathjax">\(4.1\)</span>
 of <a href="https://proceedings.mlr.press/v42/cowa14.html">this paper</a> for more details on the metric.</p>
<h3 id="-exploratory-data-analysis">○ Exploratory Data Analysis</h3>
<ul>
<li><a href="#dataset-synopsis">Dataset Synopsis</a></li>
<li><a href="#univariate-analysis">Univariate Analysis</a></li>
<li><a href="#multivariate-analysis">Multivariate Analysis</a></li>
</ul>
<h4 id="dataset-synopsis">Dataset Synopsis</h4>
<p><em>Training set synopsis</em></p>
<ul>
<li>Number of observations: <span class="has-mathjax">\(250000\)</span>
</li>
<li>Number of columns: <span class="has-mathjax">\(33\)</span>
</li>
<li>Number of integer columns: <span class="has-mathjax">\(2\)</span>
</li>
<li>Number of float columns: <span class="has-mathjax">\(30\)</span>
</li>
<li>Number of object columns: <span class="has-mathjax">\(1\)</span>
</li>
<li>Number of duplicate observations: <span class="has-mathjax">\(0\)</span>
</li>
<li>Constant columns: None</li>
<li>Number of columns with missing values: <span class="has-mathjax">\(0\)</span>
</li>
<li>Memory Usage: <span class="has-mathjax">\(62.94\)</span>
 MB</li>
</ul>
<p><em>Test set synopsis</em></p>
<ul>
<li>Number of observations: <span class="has-mathjax">\(550000\)</span>
</li>
<li>Number of columns: <span class="has-mathjax">\(31\)</span>
</li>
<li>Number of integer columns: <span class="has-mathjax">\(2\)</span>
</li>
<li>Number of float columns: <span class="has-mathjax">\(29\)</span>
</li>
<li>Number of object columns: <span class="has-mathjax">\(0\)</span>
</li>
<li>Number of duplicate observations: <span class="has-mathjax">\(0\)</span>
</li>
<li>Constant columns: None</li>
<li>Number of columns with missing values: <span class="has-mathjax">\(0\)</span>
</li>
<li>Memory Usage: <span class="has-mathjax">\(130.08\)</span>
 MB</li>
</ul>
<h4 id="univariate-analysis">Univariate Analysis</h4>
<p>The target <code>Label</code> is a <a href="https://en.wikipedia.org/wiki/Binary_data">binary variable</a>, taking values <code>b</code> and <code>s</code>, indicating the status of an event.
<div class="has-mathjax">

\begin{align*}
&\text{b} \mapsto \text{background event}\\
&\text{s} \mapsto \text{signal event}
\end{align*}

</div>
</p>
<p>We present the frequency distribution of the target variable through barplot and donutplot.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-target.png"/>
</figure>

<p>Next, we check the number of unique values taken by the predictor variables in the following set of observations:</p>
<ul>
<li>Training observations that are background events</li>
<li>Training observations that are signal events</li>
<li>All training observations</li>
<li>All test observations</li>
</ul>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-features-nunique.png"/>
</figure>

<p>We observe that a lot of predictor variables take the value <span class="has-mathjax">\(-999\)</span>
 for a significant chunk of observations. The particular value is unnatural compared to the usual values taken by the variables. We suspect that the value is used for a crude imputation of <a href="https://en.wikipedia.org/wiki/Missing_data">missing</a> or corrupted values.</p>
<p>Now, we check the proportion of the value <span class="has-mathjax">\(-999\)</span>
 in the columns of the four sets of observations mentioned earlier.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-features-minus-999.png"/>
</figure>

<p>Next, we check the distributions of the float-type features for the training set and the test set. If we are to train our model on one set (the training set) and use it to make predictions on another (the test set), then it is desirable that the distributions corresponding to the two sets have similar structure. We replace the undesirable <span class="has-mathjax">\(-999\)</span>
 values by <code>np.nan</code>, so that they do not distort the distributions.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-features-distribution-float.png"/>
</figure>

<p>Next, we compare the univariate distributions of the features for the background events and the signal events in the training set. If a feature has reasonably different distributions corresponding to the background events and the signal events, then it is a clear indication that the feature is important in the task of classifying the events when the label is unknown. Similarly, if a feature has very similar distributions for the two target classes, then it is unlikely to help in the classification problem based on the feature alone. This, however, does not take into account the possible dependence the feature may have with other features which may turn out to be useful in the task of classification. A multivariate analysis will be required to check that.</p>
<p>We present the distributions of the float features in the training set by the target class. As before, the <span class="has-mathjax">\(-999\)</span>
 values are replaced by <code>np.nan</code>, so they do not affect the distributions.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-features-distribution-float-by-target.png"/>
</figure>

<p>Now, we check <a href="https://en.wikipedia.org/wiki/Skewness">skewness</a> of the distributions of the dataset columns. The measure quantifies the asymmetry of a distribution about its mean. It is given by</p>
<div class="has-mathjax">

\[g_1 := \frac{\frac{1}{n}\sum_{i=1}^n\left(x_i-\bar{x}\right)^3}{\left\{\frac{1}{n}\sum_{i=1}^n\left(x_i-\bar{x}\right)^2\right\}^{3/2}},\]

</div>

<p>where <span class="has-mathjax">\(\bar{x}\)</span>
 is the mean of the observations, given by <span class="has-mathjax">\(\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i\).</span>
</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-features-skewness.png"/>
</figure>

<p><em>Observations</em></p>
<ul>
<li>Columns with extreme positive skewness (absolute value greater than <span class="has-mathjax">\(3\)</span>
 in the training set): <code>DER_mass_MMC</code>, <code>DER_mass_vis</code>, <code>DER_pt_tot</code>, <code>PRI_tau_pt</code>, <code>PRI_lep_pt</code>, <code>PRI_met</code>, <code>PRI_jet_subleading_pt</code></li>
<li>Columns with high positive skewness (absolute value between <span class="has-mathjax">\(1\)</span>
 and <span class="has-mathjax">\(3\)</span>
 in the training set): <code>DER_mass_transverse_met_lep</code>, <code>DER_pt_h</code>, <code>DER_mass_jet_jet</code>, <code>DER_sum_pt</code>, <code>DER_pt_ratio_lep_tau</code>, <code>PRI_met_sumet</code>, <code>PRI_jet_leading_pt</code>, <code>PRI_jet_all_pt</code></li>
<li>Columns with moderate positive skewness (absolute value between <span class="has-mathjax">\(0.5\)</span>
 and <span class="has-mathjax">\(1\)</span>
 in the training set): <code>DER_deltaeta_jet_jet</code></li>
</ul>
<p>We plot the distribution of skewness of the float features in the training set and the test set.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-features-skewness-distribution.png"/>
</figure>

<p><em>Observations</em></p>
<ul>
<li>The skewness distribution for both the training set and the test set have global peak near <span class="has-mathjax">\(0\)</span>
</li>
<li>There is a mild peak far towards the right (around <span class="has-mathjax">\(11\))</span>
 in the skewness distribution for the training set, indicating that some of the float features in the training set exhibit extremely high positive skewness</li>
<li>The skewness distribution for the test set has a clear bimodal structure, with a local peak near <span class="has-mathjax">\(2.5\)</span>
</li>
</ul>
<p>Next, we plot the distribution of skewness of the float features in the training set by target class.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-features-skewness-distribution-training-by-target.png"/>
</figure>

<p><em>Observations</em></p>
<ul>
<li>The skewness distribution for both the background events and the signal events in the training set have global peak near <span class="has-mathjax">\(0\)</span>
</li>
<li>There is a mild peak far towards the right (around <span class="has-mathjax">\(22\))</span>
 in the skewness distribution for the signal events, indicating presence of float features with extreme positive skewness for the signal events</li>
<li>The skewness distribution for the background events has a clear bimodal structure, with a local peak near <span class="has-mathjax">\(2.5\)</span>
</li>
</ul>
<p>Now, we check <a href="https://en.wikipedia.org/wiki/Kurtosis">kurtosis</a> of the distributions of the dataset columns. It generally quantifies the peakedness and tailedness of a distribution. It is given by</p>
<div class="has-mathjax">

\[b_2 = \frac{\frac{1}{n}\sum_{i=1}^n\left(x_i-\bar{x}\right)^4}{\left\{\frac{1}{n}\sum_{i=1}^n\left(x_i-\bar{x}\right)^2\right\}^2},\]

</div>

<p>where <span class="has-mathjax">\(\bar{x}\)</span>
 is the mean of the observations, given by <span class="has-mathjax">\(\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i\).</span>
 A relocated version of the kurtosis, taking into account the fact that <span class="has-mathjax">\(b_2 = 3\)</span>
 for the <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a> is defined as the <a href="https://en.wikipedia.org/wiki/Kurtosis#Excess_kurtosis">excess kurtosis</a>, given by <span class="has-mathjax">\(g_2 := b_2 - 3\).</span>
</p>
<p><em>Note</em>: The ability of the measure <span class="has-mathjax">\(b_2\)</span>
 or <span class="has-mathjax">\(g_2\)</span>
 to quantify peakedness of a distribution has been a topic of debate. See <a href="https://www.jstor.org/stable/24591697?seq=1">this paper</a> for more details.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-features-kurtosis.png"/>
</figure>

<p><em>Observations</em></p>
<ul>
<li>Columns with extreme leptokurtosis (excess kurtosis greater than <span class="has-mathjax">\(3\)</span>
 in the training set): <code>DER_mass_MMC</code>, <code>DER_mass_transverse_met_lep</code>, <code>DER_mass_vis</code>, <code>DER_pt_h</code>, <code>DER_mass_jet_jet</code>, <code>DER_pt_tot</code>, <code>DER_sum_pt</code>, <code>DER_pt_ratio_lep_tau</code>, <code>PRI_tau_pt</code>, <code>PRI_lep_pt</code>, <code>PRI_met</code>, <code>PRI_met_sumet</code>, <code>PRI_jet_leading_pt</code>, <code>PRI_jet_subleading_pt</code>, <code>PRI_jet_all_pt</code></li>
<li>Columns with high leptokurtosis (excess kurtosis between <span class="has-mathjax">\(1\)</span>
 and <span class="has-mathjax">\(3\)</span>
 in the training set): <code>DER_prodeta_jet_jet</code></li>
<li>Columns with high platykurtosis (excess kurtosis between <span class="has-mathjax">\(-3\)</span>
 and <span class="has-mathjax">\(-1\)</span>
 in the training set): <code>DER_met_phi_centrality</code>, <code>DER_lep_eta_centrality</code>, <code>PRI_tau_phi</code>, <code>PRI_lep_phi</code>, <code>PRI_met_phi</code>, <code>PRI_jet_leading_phi</code>, <code>PRI_jet_subleading_phi</code></li>
<li>Columns with moderate platykurtosis (excess kurtosis between <span class="has-mathjax">\(-1\)</span>
 and <span class="has-mathjax">\(-0.5\)</span>
 in the training set): <code>DER_deltaeta_jet_jet</code>, <code>PRI_tau_eta</code>, <code>PRI_lep_eta</code>, <code>PRI_jet_leading_eta</code>, <code>PRI_jet_subleading_eta</code></li>
</ul>
<p>Now, we plot the distribution of kurtosis of the float features in the training set by target class.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-features-kurtosis-distribution.png"/>
</figure>

<p><em>Observations</em></p>
<ul>
<li>The kurtosis distribution for both the training set and the test set have global peak near <span class="has-mathjax">\(0\)</span>
</li>
<li>The kurtosis distribution for the training set is more or less bell-shaped and relatively dispersed, whereas the same for the test set is far more concentrated about <span class="has-mathjax">\(0\)</span>
 with high peakedness</li>
</ul>
<p>Next, we plot the distribution of kurtosis of the float features in the training set by target class.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-features-kurtosis-distribution-training-by-target.png"/>
</figure>

<p><em>Observations</em></p>
<ul>
<li>The kurtosis distribution for both the background events and the signal events in the training set have global peak near <span class="has-mathjax">\(0\)</span>
</li>
<li>The kurtosis distribution for the signal events is more or less bell-shaped and relatively dispersed, whereas the same for the background events is far more concentrated about <span class="has-mathjax">\(0\)</span>
 with high peakedness</li>
</ul>
<p>We also observe that for both skewness and kurtosis, there is an uncanny resemblance between the respective distribution plots for background events in the training set and all events in the test set. This indicates that majority of the observations in the test set may be background events.</p>
<p>We now turn our attention to the only integer feature <code>PRI_jet_num</code>. First, we present frequency comparison of <code>PRI_jet_num</code> for the training set and the test set.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-pri-jet-num-training-test.png"/>
</figure>

<p><em>Observation</em>: The proportions of values of <code>PRI_jet_num</code> are more or less same for both the training set and the test set.</p>
<p>Next, we present frequency comparison of <code>PRI_jet_num</code> for the background events and the signal events in the training set.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-pri-jet-num-training-by-target.png"/>
</figure>

<p><em>Observation</em>: The proportions of values of <code>PRI_jet_num</code>, especially <span class="has-mathjax">\(0\)</span>
 and <span class="has-mathjax">\(2\),</span>
 differ for the background events and the signal events in the training set.</p>
<h4 id="multivariate-analysis">Multivariate Analysis</h4>
<p>We begin with the correlation structure among the float features. <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Correlation coefficient</a> is a statistical measure of linear dependence between two variables. Extreme correlation gives an indication that the two variables are linearly related, however this does not prove any causal relationship between the said variables. The measure is defined as the covariance of the two variables, scaled by the product of respective standard deviations. Let <span class="has-mathjax">\(\left\{\left(x_1, y_1\right), \left(x_2, y_2\right), \cdots, \left(x_n, y_n\right)\right\}\)</span>
 be paired data on the variables <span class="has-mathjax">\(\left(x, y\right)\).</span>
 Then the correlation coefficient of the two variables is given by
<div class="has-mathjax">

\[ r_{xy} := \frac{\text{cov}\left(x, y\right)}{s_x s_y} = \frac{\frac{1}{n}\sum_{i=1}^n\left(x_i - \bar{x}\right)\left(y_i - \bar{y}\right)}{\sqrt{\frac{1}{n}\sum_{i=1}^n\left(x_i - \bar{x}\right)^2} \sqrt{\frac{1}{n}\sum_{i=1}^n\left(y_i - \bar{y}\right)^2}}, \]

</div>
</p>
<p>where <span class="has-mathjax">\(\bar{x}\)</span>
 and <span class="has-mathjax">\(\bar{y}\)</span>
 denote the respective sample means of the two variables, given by <span class="has-mathjax">\(\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i\)</span>
 and <span class="has-mathjax">\(\bar{y} = \frac{1}{n}\sum_{i=1}^n y_i\).</span>
</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-correlation-df.png"/>
</figure>

<p>We plot the distributions of correlation coefficient of pairs of float features for the training set and the test set.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-correlation-distribution.png"/>
</figure>

<p><em>Observations</em></p>
<ul>
<li>The correlation distribution is almost identical for the training set and the test set</li>
<li>The density has its global peak near <span class="has-mathjax">\(0\),</span>
 however there is a local peak between <span class="has-mathjax">\(0.5\)</span>
 and <span class="has-mathjax">\(0.6\),</span>
 indicating a fair number of moderately to highly correlated pairs of features</li>
<li>There is even an increase in the density after <span class="has-mathjax">\(0.8\),</span>
 with a small peak very close to <span class="has-mathjax">\(1\),</span>
 indicating the presence of a few extremely correlated pairs of features</li>
</ul>
<p>We present the correlation heatmap of the float features in the training set.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-correlation-heatmap-training.png"/>
</figure>

<p>Next, we present the correlation heatmap of the float features in the test set.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-correlation-heatmap-test.png"/>
</figure>

<p><em>Observation</em>: The following groups have shown an extremely high positive correlation structure within themselves, i.e., any two features from a single group have an extremely high correlation coefficient.</p>
<ul>
<li><code>DER_deltaeta_jet_jet</code>, <code>DER_mass_jet_jet</code>, <code>DER_prodeta_jet_jet</code>, <code>DER_lep_eta_centrality</code>, <code>PRI_jet_subleading_pt</code>, <code>PRI_jet_subleading_eta</code>, <code>PRI_jet_subleading_phi</code></li>
<li><code>DER_sum_pt</code>, <code>PRI_met_sumet</code>, <code>PRI_jet_all_pt</code></li>
<li><code>PRI_jet_leading_pt</code>, <code>PRI_jet_leading_eta</code>, <code>PRI_jet_leading_phi</code></li>
</ul>
<p>Now, we plot the distributions of correlation coefficient of pairs of float features by target class in the training set.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-correlation-distribution-training-by-target.png"/>
</figure>

<p><em>Observations</em></p>
<ul>
<li>The left tail of the correlation distributions corresponding to background events and signal events in the training set are different</li>
<li>The densities have their respective global peaks near <span class="has-mathjax">\(0\),</span>
 however there is a local peak between <span class="has-mathjax">\(0.5\)</span>
 and <span class="has-mathjax">\(0.6\)</span>
 for both of them, indicating a fair number of pairs of float features with moderate to high positive correlation</li>
<li>The left tail of the correlation distribution for the signal events is heavier compared to the same for the background events, indicating that there are more pairs of float features with moderate to high negative correlation for the signal events than the background events in the training set</li>
<li>For both target classes, there is an increase in the density after <span class="has-mathjax">\(0.8\),</span>
 with a small peak very close to <span class="has-mathjax">\(1\),</span>
 indicating the presence of a few pairs of float features with extreme positive correlation</li>
</ul>
<p>We present the correlation heatmaps of the float features for background events and signal events in the training set.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-correlation-heatmap-training-by-target.png"/>
</figure>

<p><em>Observation</em>: The two heatmaps are more or less similar, i.e., the overall correlation structure of the float features corresponding to the background events and the signal events in the training set are very much alike.</p>
<p>In <a href="https://github.com/sugatagh/Higgs-Boson-Event-Detection/blob/main/Notebook/higgs-boson-part-1-eda.ipynb">this notebook</a>, we present bivariate (trivariate) <a href="https://en.wikipedia.org/wiki/Scatter_plot">scatterplots</a> for some selected pairs (triples) of features. These plots are given separately for the background events and signal events appearing in the training set. So they can be used not only to understand the relationship between (among) a pair (triple) of features, but also the ability of the pair (triple) to classify an event as <em>background</em> or <em>signal</em>.</p>
<p>For the sake of brevity, we do not show all the plots in this post. Instead, we give one example of bivariate scatterplot and one example of trivariate scatterplot. First, we present the bivariate scatterplot of <code>DER_mass_MMC</code> and <code>DER_prodeta_jet_jet</code>.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-bivariate-der-mass-mmc-der-prodeta-jet-jet.png"/>
</figure>

<p>Now, we present the trivariate scatterplot of <code>DER_mass_jet_jet</code>, <code>PRI_jet_leading_eta</code>, and <code>PRI_jet_subleading_eta</code>.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-trivariate-der-mass-jet-jet-et-al.png"/>
</figure>

<h3 id="-train-test-split">○ Train-Test Split</h3>
<p>The original test set does not contain <code>Weight</code> and <code>Label</code>. It is useful only in the context of <a href="https://www.kaggle.com/c/higgs-boson">this competition</a>. Thus, we cannot utilize it to evaluate the models in this project. Instead, we take a chunk out of the training set and hold it out for final evaluation.</p>
<p>Specifically, we <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">split</a> the original training set in <span class="has-mathjax">\(80:20\)</span>
 ratio. The new training set consists of <span class="has-mathjax">\(80\%\)</span>
 training data, and the new test set consists of the rest.</p>
<p>We <a href="https://en.wikipedia.org/wiki/Stratified_sampling">stratify</a> the split using <code>Label</code>, so that the proportion of each label remains roughly the same in the new training set and the new test set.</p>
<h3 id="-data-preprocessing">○ Data Preprocessing</h3>
<ul>
<li><a href="#categorical-data-encoding">Categorical Data Encoding</a></li>
<li><a href="#dropping-redundant-columns">Dropping Redundant Columns</a></li>
</ul>
<h4 id="categorical-data-encoding">Categorical Data Encoding</h4>
<p>We observe that the labels are <a href="https://en.wikipedia.org/wiki/Nominal_category">nominal</a> in nature, i.e., they do not have an inherent <a href="https://en.wikipedia.org/wiki/Order_theory">order</a>. To encode the labels, we map them to integer values in the following way: <span class="has-mathjax">\(\text{b} \mapsto 0\)</span>
 and <span class="has-mathjax">\(\text{s} \mapsto 1\).</span>
</p>
<p>The values of the <code>Label</code> column in both the training set and the test set are replaced with the corresponding encoded values.</p>
<h4 id="dropping-redundant-columns">Dropping Redundant Columns</h4>
<p>The <code>EventId</code> column has no relevance as far as the classification problem under consideration is concerned. So, we drop it from both the training set and the test set.</p>
<p>We have observed in the exploratory data analysis that <span class="has-mathjax">\(11\)</span>
 columns in the dataset contain the value <span class="has-mathjax">\(-999\),</span>
 which is very different from the other values taken by the respective features. We suspect that these are missing values filled in by a constant of arbitrary choice.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-eda-features-minus-999-modeling.png"/>
</figure>

<p>We drop these columns from the dataset. One can also experiment by imputing the <span class="has-mathjax">\(-999\)</span>
 values in the column <code>DER_mass_MMC</code>, which has a little over <span class="has-mathjax">\(15\%\)</span>
 values equal to <span class="has-mathjax">\(-999\).</span>
</p>
<p>Furthermore, we have observed in the exploratory data analysis that the following groups have shown extremely high positive correlation structure within themselves, i.e., any two features from a single group has extremely high correlation coefficient.</p>
<ul>
<li><code>DER_deltaeta_jet_jet</code>, <code>DER_mass_jet_jet</code>, <code>DER_prodeta_jet_jet</code>, <code>DER_lep_eta_centrality</code>, <code>PRI_jet_subleading_pt</code>, <code>PRI_jet_subleading_eta</code>, <code>PRI_jet_subleading_phi</code></li>
<li><code>DER_sum_pt</code>, <code>PRI_met_sumet</code>, <code>PRI_jet_all_pt</code></li>
<li><code>PRI_jet_leading_pt</code>, <code>PRI_jet_leading_eta</code>, <code>PRI_jet_leading_phi</code></li>
</ul>
<p>The first group and the last group are subsets of the dropped columns. We can explain the extremely high correlation between the pairs in those groups by the high frequency of the value <span class="has-mathjax">\(-999\).</span>
 The middle group of three features, though, does not have the same explanation. We keep only <code>DER_sum_pt</code> from these columns and drop the other two.</p>
<h3 id="-feature-engineering">○ Feature Engineering</h3>
<ul>
<li><a href="#transformation">Transformation</a></li>
<li><a href="#feature-scaling">Feature Scaling</a></li>
</ul>
<h4 id="transformation">Transformation</h4>
<p>We note the following observations from the exploratory data analysis.</p>
<ul>
<li>Columns with extreme skewness (absolute value greater than <span class="has-mathjax">\(3\)</span>
 in the dataset): <code>DER_mass_MMC</code>, <code>DER_mass_vis</code>, <code>DER_pt_tot</code>, <code>PRI_tau_pt</code>, <code>PRI_lep_pt</code>, <code>PRI_met</code>, <code>PRI_jet_subleading_pt</code></li>
<li>Columns with high skewness (absolute value between <span class="has-mathjax">\(1\)</span>
 and <span class="has-mathjax">\(3\)</span>
 in the dataset): <code>DER_mass_transverse_met_lep</code>, <code>DER_pt_h</code>, <code>DER_mass_jet_jet</code>, <code>DER_sum_pt</code>, <code>DER_pt_ratio_lep_tau</code>, <code>PRI_met_sumet</code>, <code>PRI_jet_leading_pt</code>, <code>PRI_jet_all_pt</code></li>
<li>Columns with moderate skewness (absolute value between <span class="has-mathjax">\(0.5\)</span>
 and <span class="has-mathjax">\(1\)</span>
 in the dataset): <code>DER_deltaeta_jet_jet</code></li>
</ul>
<p>In this project, we shall transform all the features with absolute value of skewness more than <span class="has-mathjax">\(0.5\).</span>
 It turns out that these columns are <code>DER_mass_transverse_met_lep</code>, <code>DER_mass_vis</code>, <code>DER_pt_h</code>, <code>DER_pt_tot</code>, <code>DER_sum_pt</code>, <code>DER_pt_ratio_lep_tau</code>, <code>PRI_tau_pt</code>, <code>PRI_lep_pt</code>, and <code>PRI_met</code>.</p>
<p>The next function transforms the selected input columns of two input DataFrames in the following manner:</p>
<ul>
<li>First, it applies location change <span class="has-mathjax">\(x \mapsto x - min(x) + 1\)</span>
 on the features to be transformed, to make their range start from <span class="has-mathjax">\(1\).</span>
 The relocated variables fall inside the range <span class="has-mathjax">\([1, \infty)\).</span>
</li>
<li>Now we apply the <a href="https://en.wikipedia.org/wiki/Logarithm">log transformation</a> <span class="has-mathjax">\(y \mapsto \log{y}\).</span>
</li>
</ul>
<pre tabindex="0"><code>def log_transform(df_train_in, df_test_in, cols_transform):
    df_train_out, df_test_out = df_train_in.copy(deep = True), df_test_in.copy(deep = True)
    for col in cols_transform:
        min_ = df_train_out[col].min()
        df_train_out[col] = df_train_out[col] - min_ + 1
        df_train_out[col] = np.log(df_train_out[col])
        df_test_out[col] = df_test_out[col] - min_ + 1
        df_test_out[col] = np.log(df_test_out[col])
    return df_train_out, df_test_out
</code></pre><p>The relocation preceding the log transformation ensures that there are no negative values to be fed to the <span class="has-mathjax">\(\log\)</span>
 function. The addition of <span class="has-mathjax">\(1\)</span>
 ensures that there are no values very close to <span class="has-mathjax">\(0\),</span>
 which the log transformation maps to extreme negative values.</p>
<p>Note that we use the same <span class="has-mathjax">\(\min(x)\)</span>
 from the training data in the test data, to keep the transformation same for the two datasets.</p>
<h4 id="feature-scaling">Feature Scaling</h4>
<p>It may be natural for one feature variable to have a greater impact on classification procedure than another. But often this is generated artificially by the difference of range of values that the features take. The <a href="https://en.wikipedia.org/wiki/Unit_of_measurement">unit of measurement</a> in which the features are measured can be one a possible reason for such occurrence.</p>
<p>To deal with this, the <a href="https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization)">min-max normalization</a> transforms the features in the following way:
<div class="has-mathjax">

\[x \mapsto \frac{x - \min{\left(x\right)}}{\max{\left(x\right)} - \min{\left(x\right)}}\]

</div>
</p>
<p>The following function applies min-max normalization on selected columns of an input training DataFrame and an input test DataFrame. To keep the transformation the same, we use the minimum and maximum values of the training columns only for both DataFrames. Using the minimum and maximum values of the test columns for both sets would have led to <a href="https://en.wikipedia.org/wiki/Leakage_(machine_learning)">data leakage</a>.</p>
<p>Specifically, we compute <span class="has-mathjax">\(\min{\left(x\right)}\)</span>
 and <span class="has-mathjax">\(\max{\left(x\right)}\)</span>
 for a particular feature from the training set only, and use those values to rescale the feature for both the training set and the test set.</p>
<pre tabindex="0"><code>def minmax_normalizer(df_train_in, df_test_in, cols):
    df_train_out, df_test_out = df_train_in.copy(deep = True), df_test_in.copy(deep = True)
    for col in cols:
        min_, max_ = df_train_out[col].min(), df_test_out[col].max()
        df_train_out[col] = (df_train_out[col] - min_) / (max_ - min_)
        df_test_out[col] = (df_test_out[col] - min_) / (max_ - min_)
    return df_train_out, df_test_out
</code></pre><p>We employ normalization on the non-constant float features in the training set and the test set.</p>
<h3 id="-baseline-model">○ Baseline Model</h3>
<ul>
<li><a href="#model-creation-and-compilation">Model Creation and Compilation</a></li>
<li><a href="#early-stopping">Early Stopping</a></li>
<li><a href="#learning-rate-scheduler">Learning Rate Scheduler</a></li>
<li><a href="#model-checkpoint">Model Checkpoint</a></li>
<li><a href="#model-fitting">Model Fitting</a></li>
<li><a href="#label-prediction">Label Prediction</a></li>
<li><a href="#model-evaluation">Model Evaluation</a></li>
</ul>
<p>First, we split both the training set and the test set into features, label, and weight.</p>
<h4 id="model-creation-and-compilation">Model Creation and Compilation</h4>
<p>The next function creates a model and compiles it. Specifically, it adds <a href="https://en.wikipedia.org/wiki/Layer_(deep_learning)#Layer_Types">dense layers</a>, with appropriate number of units and <a href="https://en.wikipedia.org/wiki/Activation_function">activation function</a>, to a <a href="https://www.tensorflow.org/guide/keras/sequential_model">sequential model</a>. Then, it compiles the model with <a href="https://www.tensorflow.org/api_docs/python/tf/keras/metrics/binary_crossentropy">binary cross-entropy</a> loss and <a href="https://arxiv.org/abs/1412.6980">Adam</a> optimizer with an input <a href="https://en.wikipedia.org/wiki/Learning_rate">learning rate</a>. The model is evaluated with the accuracy metric during the training process.</p>
<pre tabindex="0"><code>def createModel(learning_rate = 0.001):
    model = Sequential()
    model.add(Dense(units = 16, input_dim = len(X_train.columns), activation = &#39;relu&#39;))
    model.add(Dense(units = 8, activation = &#39;relu&#39;))
    model.add(Dense(units = 8, activation = &#39;relu&#39;))
    model.add(Dense(units = 1, activation = &#39;sigmoid&#39;))
    model.compile(loss = &#39;binary_crossentropy&#39;,
                  optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),
                  metrics = [&#39;accuracy&#39;])
    return model
</code></pre><p>We apply the function and report the summary of the model.</p>
<pre tabindex="0"><code>model = createModel()
model.summary()
</code></pre><figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-nn-baseline-model-summary.png"/>
</figure>

<h4 id="early-stopping">Early Stopping</h4>
<p>The <a href="https://en.wikipedia.org/wiki/Early_stopping">early stopping</a> callback is useful in stopping the training process once it reaches a certain level of stagnation.</p>
<p>We create an instance of the callback, which monitors the validation loss and stops the training process if the validation loss does not change beyond the amount <span class="has-mathjax">\(0.001\)</span>
 for <span class="has-mathjax">\(20\)</span>
 epochs.</p>
<pre tabindex="0"><code>earlystop = EarlyStopping(
    monitor = &#39;val_loss&#39;,
    min_delta = 0.001,
    patience = 20,
    verbose = 0,
    mode = &#39;auto&#39;,
    start_from_epoch = 60
)
</code></pre><h4 id="learning-rate-scheduler">Learning Rate Scheduler</h4>
<p>A <a href="https://keras.io/api/callbacks/learning_rate_scheduler/">learning rate scheduler</a> modifies the learning rate during the training process, frequently decreasing it as the training goes on. This allows better fine-tuning by enabling the model to make larger updates early in the training process when the parameters are far from optimal and smaller updates later on when the parameters are closer to optimal.</p>
<p>At the beginning of every epoch, this callback gets the updated learning rate value from a <em>schedule function</em>, with the current epoch and current learning rate, and applies the updated learning rate on the optimizer. The schedule function takes an epoch index (integer, indexed from 0) and current learning rate (float) as inputs and returns a new learning rate as output (float).</p>
<p><strong>Modified linear schedule function.</strong> A common choice of scheduler is <em>linear decay</em>. In what follows, we manually set the total number of epochs to <span class="has-mathjax">\(100\).</span>
 This can be modified as per requirement. Thus, unless interrupted by the early stopping callback, the training process runs from <code>epoch = 0</code> to <code>epoch = 99</code>. The following function implements the linear decay, where the learning rate linearly decreases as the training process approaches the <span class="has-mathjax">\(100\)th</span>
 epoch.</p>
<pre tabindex="0"><code>def scheduler_linear(epoch, learning_rate):
    if epoch &lt; 1:
        return learning_rate
    else:
        return learning_rate * (100 - epoch - 1) / (100 - epoch)
</code></pre><p>The next function modifies the scheduler by keeping the learning rate constant for the first <span class="has-mathjax">\(40\)</span>
 epochs and then gradually decreasing it at a linear rate as it approaches the <span class="has-mathjax">\(100\)th</span>
 epoch. Note that we manually set the initial number of epochs (when the learning rate remains the same) to <span class="has-mathjax">\(40\).</span>
 This can be changed as per need.</p>
<pre tabindex="0"><code>def scheduler_modified_linear(epoch, learning_rate):
    if epoch &lt; 40:
        return learning_rate
    else:
        return learning_rate * (100 - epoch) / (100 - epoch + 1)
</code></pre><p>We compare the two schedule function by plotting learning rate against epoch for both setup.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-nn-learning-rate-modified-linear-decay.png"/>
</figure>

<p><strong>Modified exponential schedule function.</strong> Another common choice of scheduler is <em>exponential decay</em>. The following function implements the scheduler, which decreases the learning rate exponentially as the training process approaches the <span class="has-mathjax">\(100\)th</span>
 epoch.</p>
<pre tabindex="0"><code>def scheduler_exponential(epoch, learning_rate):
    if epoch &lt; 1:
        return learning_rate
    else:
        return learning_rate * math.exp(-0.1)
</code></pre><p>The next function modifies the scheduler by keeping the learning rate constant for the first <span class="has-mathjax">\(40\)</span>
 epochs and then gradually decreasing it at an exponential rate as it approaches the <span class="has-mathjax">\(100\)th</span>
 epoch.</p>
<pre tabindex="0"><code>def scheduler_modified_linear(epoch, learning_rate):
    if epoch &lt; 40:
        return learning_rate
    else:
        return learning_rate * (100 - epoch) / (100 - epoch + 1)
</code></pre><p>We compare the two schedule function by plotting learning rate against epoch for both setup.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-nn-learning-rate-modified-exponential-decay.png"/>
</figure>

<p>In this project, we use the modified linear decay.</p>
<pre tabindex="0"><code>learning_rate_scheduler = LearningRateScheduler(scheduler_modified_linear)
</code></pre><h4 id="model-checkpoint">Model Checkpoint</h4>
<p>The <a href="https://keras.io/api/callbacks/model_checkpoint/">ModelCheckpoint</a> callback saves a <a href="https://keras.io/">Keras</a> model or model weights at some frequency. It works together with training using <code>model.fit()</code> to save a model or weights (in a checkpoint file) at certain interval. This allows the model or weights to be loaded at a later time to resume training from the saved state.</p>
<p>Here, we only keep the model that has achieved the best performance so far, in terms of validation loss, instead of saving the model at the end of every epoch regardless of performance.</p>
<pre tabindex="0"><code>model_checkpoint = ModelCheckpoint(
    filepath = &#39;best_model.h5&#39;,
    monitor = &#39;val_loss&#39;,
    save_best_only = True
)
</code></pre><h4 id="model-fitting">Model Fitting</h4>
<p>We incorporate the three callbacks described above in training the model for <span class="has-mathjax">\(100\)</span>
 epochs, taking <span class="has-mathjax">\(32\)</span>
 observations per gradient update.</p>
<p>Specifically, we set aside the last <span class="has-mathjax">\(20\%\)</span>
 observations from the training set to constitute the validation set. The rest constitutes the effective training set. In each epoch, the model is trained over the effective training set and is evaluated on the validation set.</p>
<pre tabindex="0"><code>history = model.fit(
    X_train,
    y = y_train,
    batch_size = 32,
    epochs = 100,
    validation_split = 0.2,
    callbacks = [earlystop, learning_rate_scheduler, model_checkpoint],
    verbose = 0
)
</code></pre><p>We visualize the model loss (binary cross-entropy) for the training set and the validation set over the epochs.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-nn-baseline-model-loss.png"/>
</figure>

<p>Furthermore, we visualize model accuracy for the training set and the validation set over the epochs.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-nn-baseline-model-accuracy.png"/>
</figure>

<h4 id="label-prediction">Label Prediction</h4>
<p>We load the best model, which is saved by the <code>ModelCheckpoint</code> callback, to predict the label <span class="has-mathjax">\(1\)</span>
 (signal event) probabilities. Then, we convert them to binary predictions <span class="has-mathjax">(\(0\)</span>
 or <span class="has-mathjax">\(1\)).</span>
</p>
<div class="has-mathjax">

\begin{align*}
&\text{probability} < 0.5 \mapsto y = 0 \,\,\text{(background event)}\\
&\text{probability} \geq 0.5 \mapsto y = 1 \,\,\text{(signal event)}
\end{align*}

</div>

<h4 id="model-evaluation">Model Evaluation</h4>
<p>We compute approximate median significance (AMS) for predictions on the training set and the test set.</p>
<ul>
<li>Training AMS: <span class="has-mathjax">\(2.509776\)</span>
</li>
<li>Test AMS: <span class="has-mathjax">\(1.214260\)</span>
</li>
</ul>
<h3 id="-hyperparameter-tuning">○ Hyperparameter Tuning</h3>
<ul>
<li><a href="#keras-model">Keras Model</a></li>
<li><a href="#grid-search">Grid Search</a></li>
<li><a href="#best-model-instance">Best Model Instance</a></li>
</ul>
<h4 id="keras-model">Keras Model</h4>
<p>We employ the <a href="https://en.wikipedia.org/wiki/Scikit-learn">scikit-learn</a> classifier API for Keras, <code>KerasClassifier</code>, to create and compile the model.</p>
<p>The <code>createModel</code> function, defined earlier, is used to build the Keras model, compiled using the Adam optimizer and binary cross-entropy loss. The model is set to train for <span class="has-mathjax">\(100\)</span>
 epochs, taking <span class="has-mathjax">\(32\)</span>
 observations per gradient update. As before, we incorporate the three callbacks: <code>EarlyStopping</code>, <code>LearningRateScheduler</code>, and <code>ModelCheckpoint</code>. We fix the <code>random_state</code> parameter for reproducibility of the results.</p>
<pre tabindex="0"><code>model_ = KerasClassifier(
    model = createModel,
    optimizer = &#39;adam&#39;,
    loss = &#39;binary_crossentropy&#39;,
    batch_size = 32,
    epochs = 100,
    learning_rate = 0.001,
    callbacks = [earlystop, learning_rate_scheduler, model_checkpoint],
    random_state = 0,
    verbose = 0
)
</code></pre><h4 id="grid-search">Grid Search</h4>
<p>We employ the traditional <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search">grid search</a> technique for hyperparameter tuning. We shall focus on tuning two specific parameters, namely <code>batch_size</code> and <code>learning_rate</code>. The following dictionary contains the specific values of these parameters during the grid search.</p>
<pre tabindex="0"><code>param_grid = {
    &#39;batch_size&#39;   : [32, 64, 128],
    &#39;learning_rate&#39;: [0.001, 0.003, 0.01]
}
</code></pre><p>We feed this to the <code>GridSearchCV</code> class, which trains the model from each of the <span class="has-mathjax">\(3 \times 3 = 9\)</span>
 instances in <code>param_grid</code>. The <span class="has-mathjax">\(5\)-fold</span>
 cross-validated models are evaluated by the accuracy metric and the best model is refit to the entire training set.</p>
<h4 id="best-model-instance">Best Model Instance</h4>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-nn-ht-grid-search-summary.png"/>
</figure>

<p>The optimal values of the hyperparameters are given as follows.</p>
<ul>
<li><code>batch_size</code>: <span class="has-mathjax">\(128\)</span>
</li>
<li><code>learning_rate</code>: <span class="has-mathjax">\(0.003\)</span>
</li>
</ul>
<p>The best cross-validation score, which is <span class="has-mathjax">\(5\)-fold</span>
 cross-validation accuracy in this case, is <span class="has-mathjax">\(0.82782\).</span>
</p>
<h3 id="-prediction-and-evaluation">○ Prediction and Evaluation</h3>
<p>We predict the label <span class="has-mathjax">\(1\)</span>
 (signal event) probabilities using the tuned model and convert the predicted probabilities to labels. The AMS scores for predictions on the training set and the test set are computed.</p>
<ul>
<li>Training AMS: <span class="has-mathjax">\(2.500144\)</span>
</li>
<li>Test AMS: <span class="has-mathjax">\(1.200022\)</span>
</li>
</ul>
<p>Observe that the test AMS for the tuned model is actually less than that of the baseline model, which may prompt one to suspect that it is a poorer model. However, it is not the case, as the candidates for the tuned model were evaluated under a cross-validation setup where the final model has the highest cross-validation score, exceeding that of the baseline model.</p>
<p>While the performance (in terms of AMS score) does dip very slightly on the particular test set, we aim to produce a model that generalizes best to new data. For that reason, we go with the model that gives the best cross-validation score.</p>
<p>Next, we compute accuracy scores for predictions on the training set and the test set. Note that accuracy does not consider <code>Weight</code> of the observations.</p>
<ul>
<li>Training accuracy: <span class="has-mathjax">\(0.827070\)</span>
</li>
<li>Test accuracy: <span class="has-mathjax">\(0.824100\)</span>
</li>
</ul>
<p>Finally, we produce a <a href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a> that depicts the performance of the final model on the test set.</p>
<figure><img src="https://sugatagh.github.io/dsml/images/images-projects/hbed/hbed-nn-ht-confusion-matrix.png"/>
</figure>

<h3 id="-acknowledgements">○ Acknowledgements</h3>
<ul>
<li><a href="http://opendata.cern.ch/record/328">Dataset from the ATLAS Higgs Boson Machine Learning Challenge <span class="has-mathjax">\(2014\)</span>
</a></li>
<li><a href="https://www.kaggle.com/competitions/higgs-boson/data">Higgs Boson Machine Learning Challenge dataset</a></li>
<li><a href="https://higgsml.ijclab.in2p3.fr/documentation/">The Higgs boson machine learning challenge documentation</a></li>
</ul>
<h3 id="-references">○ References</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Activation_function">Activation function</a></li>
<li><a href="https://arxiv.org/abs/1412.6980">Adam optimizer</a></li>
<li><a href="https://home.cern/science/experiments/atlas">ATLAS</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/metrics/binary_crossentropy">Binary cross-entropy</a></li>
<li><a href="https://en.wikipedia.org/wiki/Boson">Boson</a></li>
<li><a href="https://en.wikipedia.org/wiki/CERN">CERN</a></li>
<li><a href="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion matrix</a></li>
<li><a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Correlation coefficient</a></li>
<li><a href="https://en.wikipedia.org/wiki/Leakage_(machine_learning)">Data leakage</a></li>
<li><a href="https://en.wikipedia.org/wiki/Deep_learning">Deep learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks">Deep neural networks</a></li>
<li><a href="https://en.wikipedia.org/wiki/Layer_(deep_learning)#Layer_Types">Dense layers</a></li>
<li><a href="https://en.wikipedia.org/wiki/Early_stopping">Early stopping</a></li>
<li><a href="https://en.wikipedia.org/wiki/Elementary_particle">Elementary particle</a></li>
<li><a href="https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers">Evaluation of binary classifiers</a></li>
<li><a href="https://cds.cern.ch/record/1632191">Evidence for Higgs Boson Decays to the <span class="has-mathjax">\(\tau^+\tau^-\)</span>
 Final State with the ATLAS Detector</a></li>
<li><a href="https://en.wikipedia.org/wiki/Kurtosis#Excess_kurtosis">Excess kurtosis</a></li>
<li><a href="https://en.wikipedia.org/wiki/Exploratory_data_analysis">Exploratory data analysis</a></li>
<li><a href="https://en.wikipedia.org/wiki/False_positive_rate">False positive rate</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fermion">Fermion</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fundamental_interaction">Fundamental interaction</a></li>
<li><a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search">Grid search</a></li>
<li><a href="https://en.wikipedia.org/wiki/Higgs_boson">Higgs boson</a></li>
<li><a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">Hyperparameter optimization</a></li>
<li><a href="https://en.wikipedia.org/wiki/Indicator_function">Indicator function</a></li>
<li><a href="https://keras.io/">Keras</a></li>
<li><a href="https://en.wikipedia.org/wiki/Kurtosis">Kurtosis</a></li>
<li><a href="https://www.jstor.org/stable/24591697">Kurtosis as Peakedness, <span class="has-mathjax">\(1905–2014\).</span>
 R.I.P.</a></li>
<li><a href="https://en.wikipedia.org/wiki/Learning_rate">Learning rate</a></li>
<li><a href="https://keras.io/api/callbacks/learning_rate_scheduler/">Learning rate scheduler</a></li>
<li><a href="https://en.wikipedia.org/wiki/Logarithm">Logarithm</a></li>
<li><a href="https://en.wikipedia.org/wiki/Machine_learning">Machine learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization)">Min-max normalization</a></li>
<li><a href="https://en.wikipedia.org/wiki/Missing_data">Missing data</a></li>
<li><a href="https://keras.io/api/callbacks/model_checkpoint/">Model checkpoint</a></li>
<li><a href="https://en.wikipedia.org/wiki/Natural_logarithm">Natural logarithm</a></li>
<li><a href="https://en.wikipedia.org/wiki/Neural_network">Neural network</a></li>
<li><a href="https://en.wikipedia.org/wiki/Nominal_category">Nominal category</a></li>
<li><a href="https://en.wikipedia.org/wiki/Normal_distribution">Normal distribution</a></li>
<li><a href="https://en.wikipedia.org/wiki/Order_theory">Order theory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Particle_accelerator">Particle accelerator</a></li>
<li><a href="https://en.wikipedia.org/wiki/Particle_beam">Particle beam</a></li>
<li><a href="https://en.wikipedia.org/wiki/Particle_decay">Particle decay</a></li>
<li><a href="https://en.wikipedia.org/wiki/Particle_physics">Particle physics</a></li>
<li><a href="https://en.wikipedia.org/wiki/Scikit-learn">Scikit-learn</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">Sensitivity and specificity</a></li>
<li><a href="https://www.tensorflow.org/guide/keras/sequential_model">Sequential model</a></li>
<li><a href="https://en.wikipedia.org/wiki/Skewness">Skewness</a></li>
<li><a href="https://en.wikipedia.org/wiki/Stratified_sampling">Stratified sampling</a></li>
<li><a href="https://en.wikipedia.org/wiki/Subatomic_particle">Subatomic particle</a></li>
<li><a href="https://simple.wikipedia.org/wiki/Tau_lepton">Tau lepton</a></li>
<li><a href="https://higgsml.ijclab.in2p3.fr/">The HiggsML challenge</a></li>
<li><a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets#Test_data_set">Test dataset</a></li>
<li><a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets#Training_data_set">Training dataset</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">Train-test split</a></li>
</ul>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://sugatagh.github.io/dsml/" >
    &copy;  Sugata Ghosh, PhD 2024 
  </a>
    <div>
<div class="ananke-socials">
  
    
    <a href="https://www.linkedin.com/in/sugataghosh/" target="_blank" rel="noopener" class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" aria-label="follow on LinkedIn——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
    
    <a href="https://twitter.com/sugatagh" target="_blank" rel="noopener" class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Twitter link" aria-label="follow on Twitter——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
    
    <a href="https://github.com/sugatagh" target="_blank" rel="noopener" class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" aria-label="follow on GitHub——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
    
    <a href="https://www.kaggle.com/sugataghosh" target="_blank" rel="noopener" class="Kaggle ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Kaggle link" aria-label="follow on Kaggle——Opens in a new window">
      
        Kaggle
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
